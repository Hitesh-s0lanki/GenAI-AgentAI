{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68b138a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://docs.smith.langchain.com/observability\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "717bcabf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73aa81be",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "## langsmith Tracking \n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = os.getenv(\"LANGCHAIN_PROJECT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6baa69d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "loader = WebBaseLoader(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08a8d46b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.document_loaders.web_base.WebBaseLoader at 0x27d6efef310>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fec74e8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://docs.smith.langchain.com/observability', 'title': 'Observability Quick Start | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'This tutorial will get you up and running with our observability SDK by showing you how to', 'language': 'en'}, page_content='\\n\\n\\n\\n\\nObservability Quick Start | ü¶úÔ∏èüõ†Ô∏è LangSmith\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to main contentWe are growing and hiring for multiple roles for LangChain, LangGraph and LangSmith. Join our team!API ReferenceRESTPythonJS/TSSearchRegionUSEUGo to AppGet StartedObservabilityQuick StartTutorialsAdd observability to your LLM applicationHow-to GuidesAnnotate code for tracingFilter traces in the applicationUpload files with tracesDashboardsLog traces to specific projectSet up automation rulesOnline EvaluationSet a sampling rate for tracesAdd metadata and tags to tracesImplement distributed tracingAccess the current run (span) within a traced functionLog multimodal tracesLog retriever tracesLog custom LLM tracesPrevent logging of sensitive data in tracesQuery tracesShare or unshare a trace publiclyCompare tracesTrace generator functionsTrace with LangChain (Python and JS/TS)Trace with LangGraph (Python and JS/TS)Trace with Instructor (Python only)Trace with OpenTelemetryTrace with the Vercel AI SDK (JS/TS only)Trace without setting environment variablesTrace using the LangSmith REST APITrace with OpenAI Agents SDKCalculate token-based costs for tracesTroubleshoot trace nestingBulk Exporting Trace DataAlerts in LangSmithConfiguring PagerDuty Integration for LangSmith AlertsConfiguring Webhook Notifications for LangSmith Alerts[Beta] LangSmith Collector-ProxyHow to print detailed logs (Python SDK)Trace JS functions in serverless environmentsSet up threadsToubleshooting variable cachingTrace LangChain with OpenTelemetrySet up webhook notifications for rulesConceptual GuideEvaluationPrompt EngineeringDeployment (LangGraph Platform)AdministrationSelf-hostingPricingReferenceCloud architecture and scalabilityAuthz and AuthnAuthentication methodsdata_formatsEvaluationDataset transformationsRegions FAQsdk_referenceChangelogCloud architecture and scalabilityAuthz and AuthnAuthentication methodsdata_formatsEvaluationDataset transformationsRegions FAQsdk_referenceObservabilityOn this pageObservability Quick Start\\nThis tutorial will get you up and running with our observability SDK by showing you how to\\ntrace your application to LangSmith.\\nIf you\\'re already familiar with the observability SDK, or are interested in tracing more than just\\nLLM calls you can skip to the next steps section,\\nor check out the how-to guides.\\nTrace LangChain or LangGraph ApplicationsIf you are using LangChain or LangGraph, which both integrate seamlessly with LangSmith,\\nyou can get started by reading the guides for tracing with LangChain or tracing with LangGraph.\\n1. Install Dependencies\\u200b\\nPythonTypeScriptpip install -U langsmith openaiyarn add langsmith openai\\n2. Create an API key\\u200b\\nTo create an API key head to the LangSmith settings page. Then click Create API Key.\\n3. Set up your environment\\u200b\\nShellexport LANGSMITH_TRACING=trueexport LANGSMITH_API_KEY=\"<your-langsmith-api-key>\"# The example uses OpenAI, but it\\'s not necessary if your code uses another LLM providerexport OPENAI_API_KEY=\"<your-openai-api-key>\"\\n4. Define your application\\u200b\\nWe will instrument a simple RAG\\napplication for this tutorial, but feel free to use your own code if you\\'d like - just make sure\\nit has an LLM call!\\nApplication CodePythonTypeScriptfrom openai import OpenAIopenai_client = OpenAI()# This is the retriever we will use in RAG# This is mocked out, but it could be anything we wantdef retriever(query: str):    results = [\"Harrison worked at Kensho\"]    return results# This is the end-to-end RAG chain.# It does a retrieval step then calls OpenAIdef rag(question):    docs = retriever(question)    system_message = \"\"\"Answer the users question using only the provided information below:        {docs}\"\"\".format(docs=\"\\\\n\".join(docs))        return openai_client.chat.completions.create(        messages=[            {\"role\": \"system\", \"content\": system_message},            {\"role\": \"user\", \"content\": question},        ],        model=\"gpt-4o-mini\",    )import { OpenAI } from \"openai\";const openAIClient = new OpenAI();// This is the retriever we will use in RAG// This is mocked out, but it could be anything we wantasync function retriever(query: string) {  return [\"This is a document\"];}// This is the end-to-end RAG chain.// It does a retrieval step then calls OpenAIasync function rag(question: string) {  const docs = await retriever(question);    const systemMessage =    \"Answer the users question using only the provided information below:\\\\n\\\\n\" +    docs.join(\"\\\\n\");      return await openAIClient.chat.completions.create({    messages: [      { role: \"system\", content: systemMessage },      { role: \"user\", content: question },    ],    model: \"gpt-4o-mini\",  });}\\n5. Trace OpenAI calls\\x00\\x00\\u200b\\nThe first thing you might want to trace is all your OpenAI calls. LangSmith makes this easy with the wrap_openai (Python) or wrapOpenAI (TypeScript) wrappers.\\nAll you have to do is modify your code to use the wrapped client instead of using the OpenAI client directly.\\nPythonTypeScriptfrom openai import OpenAIfrom langsmith.wrappers import wrap_openaiopenai_client = wrap_openai(OpenAI())# This is the retriever we will use in RAG# This is mocked out, but it could be anything we wantdef retriever(query: str):    results = [\"Harrison worked at Kensho\"]    return results# This is the end-to-end RAG chain.# It does a retrieval step then calls OpenAIdef rag(question):    docs = retriever(question)    system_message = \"\"\"Answer the users question using only the provided information below:        {docs}\"\"\".format(docs=\"\\\\n\".join(docs))        return openai_client.chat.completions.create(        messages=[            {\"role\": \"system\", \"content\": system_message},            {\"role\": \"user\", \"content\": question},        ],        model=\"gpt-4o-mini\",    )import { OpenAI } from \"openai\";import { wrapOpenAI } from \"langsmith/wrappers\";const openAIClient = wrapOpenAI(new OpenAI());// This is the retriever we will use in RAG// This is mocked out, but it could be anything we wantasync function retriever(query: string) {  return [\"This is a document\"];}// This is the end-to-end RAG chain.// It does a retrieval step then calls OpenAIasync function rag(question: string) {  const docs = await retriever(question);    const systemMessage =    \"Answer the users question using only the provided information below:\\\\n\\\\n\" +    docs.join(\"\\\\n\");      return await openAIClient.chat.completions.create({    messages: [      { role: \"system\", content: systemMessage },      { role: \"user\", content: question },    ],    model: \"gpt-4o-mini\",  });}\\nNow when you call your application as follows:\\nrag(\"where did harrison work\")\\nThis will produce a trace of just the OpenAI call in LangSmith\\'s default tracing project. It should look something like this.\\n\\n6. Trace entire application\\u200b\\nYou can also use the traceable decorator (Python or TypeScript) to trace your entire application instead of just the LLM calls.\\nPythonTypeScriptfrom openai import OpenAIfrom langsmith import traceablefrom langsmith.wrappers import wrap_openaiopenai_client = wrap_openai(OpenAI())def retriever(query: str):    results = [\"Harrison worked at Kensho\"]    return results@traceabledef rag(question):    docs = retriever(question)    system_message = \"\"\"Answer the users question using only the provided information below:        {docs}\"\"\".format(docs=\"\\\\n\".join(docs))        return openai_client.chat.completions.create(        messages=[            {\"role\": \"system\", \"content\": system_message},            {\"role\": \"user\", \"content\": question},        ],        model=\"gpt-4o-mini\",    )import { OpenAI } from \"openai\";import { traceable } from \"langsmith/traceable\";import { wrapOpenAI } from \"langsmith/wrappers\";const openAIClient = wrapOpenAI(new OpenAI());async function retriever(query: string) {  return [\"This is a document\"];}const rag = traceable(async function rag(question: string) {  const docs = await retriever(question);    const systemMessage =    \"Answer the users question using only the provided information below:\\\\n\\\\n\" +    docs.join(\"\\\\n\");      return await openAIClient.chat.completions.create({    messages: [      { role: \"system\", content: systemMessage },      { role: \"user\", content: question },    ],    model: \"gpt-4o-mini\",  });});\\nNow if you call your application as follows:\\nrag(\"where did harrison work\")\\nThis will produce a trace of just the entire pipeline (with the OpenAI call as a child run) - it should look something like this\\n\\nNext steps\\u200b\\nCongratulations! If you\\'ve made it this far, you\\'re well on your way to being an expert in observability with LangSmith.\\nHere are some topics you might want to explore next:\\n\\nTrace multiturn conversations\\nSend traces to a specific project\\nFilter traces in a project\\n\\nOr you can visit the how-to guides page to find out about all the things you can do with LangSmith observability.\\nIf you prefer a video tutorial, check out the Tracing Basics video from the Introduction to LangSmith Course.Was this page helpful?You can leave detailed feedback on GitHub.PreviousGet StartedNextQuick Start1. Install Dependencies2. Create an API key3. Set up your environment4. Define your application5. Trace OpenAI calls6. Trace entire applicationNext stepsCommunityTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright ¬© 2025 LangChain, Inc.\\n\\n')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = loader.load()\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2566fc3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Text Chunk \n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d7ae09b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_docs = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "28ec1d88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(final_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d83208fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ef18feef",
   "metadata": {},
   "outputs": [],
   "source": [
    "## local vector stroe DB\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "vectorStore = FAISS.from_documents(final_docs, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "20ce8440",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x27d7ed5d720>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b61b473f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"4. Define your application\\u200b\\nWe will instrument a simple RAG\\napplication for this tutorial, but feel free to use your own code if you'd like - just make sure\\nit has an LLM call!\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Query From a vector db\n",
    "query = \"We will instrument a simple RAG application for this tutorial, but feel free to use your own code if you'd like - just make sure it has an LLM call!\"\n",
    "\n",
    "result = vectorStore.similarity_search(query)\n",
    "result[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0b0aa745",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x0000027D63602CB0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x0000027D653A7D30>, root_client=<openai.OpenAI object at 0x0000027D6516D2D0>, root_async_client=<openai.AsyncOpenAI object at 0x0000027D636013C0>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o\")\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4503e027",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template='\\n        Answer the following questiobn based only on the provided context\\n\\n        <context>\\n        {context} \\n        </context>\\n    '), additional_kwargs={})])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Retrieval Chain, Document chain\n",
    "\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "        Answer the following questiobn based only on the provided context\n",
    "\n",
    "        <context>\n",
    "        {context} \n",
    "        </context>\n",
    "    \"\"\"\n",
    ")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a846a738",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableLambda(format_docs)\n",
       "}), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
       "| ChatPromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template='\\n        Answer the following questiobn based only on the provided context\\n\\n        <context>\\n        {context} \\n        </context>\\n    '), additional_kwargs={})])\n",
       "| ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x0000027D63602CB0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x0000027D653A7D30>, root_client=<openai.OpenAI object at 0x0000027D6516D2D0>, root_async_client=<openai.AsyncOpenAI object at 0x0000027D636013C0>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********'))\n",
       "| StrOutputParser(), kwargs={}, config={'run_name': 'stuff_documents_chain'}, config_factories=[])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_chain = create_stuff_documents_chain(llm, prompt)\n",
    "document_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "65eae1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "response = document_chain.invoke({\n",
    "    \"input\":\"Trace OpenAI calls\",\n",
    "    \"context\":[Document(page_content=\"The first thing you might want to trace is all your OpenAI calls. LangSmith makes this easy with the wrap_openai (Python) or wrapOpenAI (TypeScript) wrappers. All you have to do is modify your code to use the wrapped client instead of using the OpenAI client directly.\")]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c4052bb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'To trace your OpenAI calls, you can use the LangSmith wrappers: `wrap_openai` for Python or `wrapOpenAI` for TypeScript. Modify your code to use the wrapped client instead of the OpenAI client directly.'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9c33278c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x27d7ed5d720>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Retriever -> interface no need to do similarity serach\n",
    "\n",
    "vectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e70b1530",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorStore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "81820659",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "\n",
    "retrieve_chain = create_retrieval_chain(retriever, document_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "088601ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableBinding(bound=RunnableLambda(lambda x: x['input'])\n",
       "           | VectorStoreRetriever(tags=['FAISS', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x0000027D7ED5D720>, search_kwargs={}), kwargs={}, config={'run_name': 'retrieve_documents'}, config_factories=[])\n",
       "})\n",
       "| RunnableAssign(mapper={\n",
       "    answer: RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "              context: RunnableLambda(format_docs)\n",
       "            }), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
       "            | ChatPromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template='\\n        Answer the following questiobn based only on the provided context\\n\\n        <context>\\n        {context} \\n        </context>\\n    '), additional_kwargs={})])\n",
       "            | ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x0000027D63602CB0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x0000027D653A7D30>, root_client=<openai.OpenAI object at 0x0000027D6516D2D0>, root_async_client=<openai.AsyncOpenAI object at 0x0000027D636013C0>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********'))\n",
       "            | StrOutputParser(), kwargs={}, config={'run_name': 'stuff_documents_chain'}, config_factories=[])\n",
       "  }), kwargs={}, config={'run_name': 'retrieval_chain'}, config_factories=[])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieve_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7593c628",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"How can you trace OpenAI calls using LangSmith according to the provided context?\\n\\nYou can trace OpenAI calls using LangSmith by modifying your code to use the `wrap_openai` (Python) or `wrapOpenAI` (TypeScript) wrappers. By doing this, you'll replace the direct use of the OpenAI client with the wrapped client, enabling the tracing of OpenAI calls.\""
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = retrieve_chain.invoke({\n",
    "    \"input\":\"Trace OpenAI calls\"\n",
    "})\n",
    "\n",
    "response['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "eabb98dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'Trace OpenAI calls',\n",
       " 'context': [Document(id='96a99d78-0308-4480-9dd9-3944ac4a7d05', metadata={'source': 'https://docs.smith.langchain.com/observability', 'title': 'Observability Quick Start | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'This tutorial will get you up and running with our observability SDK by showing you how to', 'language': 'en'}, page_content='5. Trace OpenAI calls\\x00\\x00\\u200b\\nThe first thing you might want to trace is all your OpenAI calls. LangSmith makes this easy with the wrap_openai (Python) or wrapOpenAI (TypeScript) wrappers.\\nAll you have to do is modify your code to use the wrapped client instead of using the OpenAI client directly.'),\n",
       "  Document(id='30bea452-8618-42a9-b43a-35ad22f52ac8', metadata={'source': 'https://docs.smith.langchain.com/observability', 'title': 'Observability Quick Start | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'This tutorial will get you up and running with our observability SDK by showing you how to', 'language': 'en'}, page_content='Now if you call your application as follows:\\nrag(\"where did harrison work\")\\nThis will produce a trace of just the entire pipeline (with the OpenAI call as a child run) - it should look something like this'),\n",
       "  Document(id='49b4181e-e950-438c-a8b7-cb08815ff2f7', metadata={'source': 'https://docs.smith.langchain.com/observability', 'title': 'Observability Quick Start | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'This tutorial will get you up and running with our observability SDK by showing you how to', 'language': 'en'}, page_content='Now when you call your application as follows:\\nrag(\"where did harrison work\")\\nThis will produce a trace of just the OpenAI call in LangSmith\\'s default tracing project. It should look something like this.'),\n",
       "  Document(id='8d6aef5c-8733-4a45-9551-5769c1840565', metadata={'source': 'https://docs.smith.langchain.com/observability', 'title': 'Observability Quick Start | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'This tutorial will get you up and running with our observability SDK by showing you how to', 'language': 'en'}, page_content='openAIClient = wrapOpenAI(new OpenAI());async function retriever(query: string) {  return [\"This is a document\"];}const rag = traceable(async function rag(question: string) {  const docs = await retriever(question);    const systemMessage =    \"Answer the users question using only the provided information below:\\\\n\\\\n\" +    docs.join(\"\\\\n\");      return await openAIClient.chat.completions.create({    messages: [      { role: \"system\", content: systemMessage },      { role: \"user\", content: question },    ],    model: \"gpt-4o-mini\",  });});')],\n",
       " 'answer': \"How can you trace OpenAI calls using LangSmith according to the provided context?\\n\\nYou can trace OpenAI calls using LangSmith by modifying your code to use the `wrap_openai` (Python) or `wrapOpenAI` (TypeScript) wrappers. By doing this, you'll replace the direct use of the OpenAI client with the wrapped client, enabling the tracing of OpenAI calls.\"}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b598a42",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
